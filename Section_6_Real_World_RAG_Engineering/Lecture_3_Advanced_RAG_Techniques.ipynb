{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c27d377",
   "metadata": {},
   "source": [
    "# üìò Advanced RAG Techniques and Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e057ba7",
   "metadata": {},
   "source": [
    "### üõ† Step 1: Setup ‚Äî Load Libraries and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9fb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81882c59",
   "metadata": {},
   "source": [
    "### üìö Step 2: Prepare the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8076f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"LangChain is a framework to build applications powered by LLMs.\",\n",
    "    \"LangChain supports agents, tools, and RAG workflows.\",\n",
    "    \"LangChain enables document-based Q&A systems with FAISS and OpenAI.\",\n",
    "    \"To use RAG, you must first index your documents using embeddings.\",\n",
    "    \"Document chunking helps improve retrieval accuracy by dividing text into smaller units.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc628c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6a180",
   "metadata": {},
   "source": [
    "### üß† Step 3: Create the Retrieval-Enhanced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffc9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab6ed7",
   "metadata": {},
   "source": [
    "### ‚ùì Step 4: Ask a Question ‚Äî and Analyze the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01369271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangChain supports document-based Q&A systems by integrating FAISS and OpenAI. This allows for efficient and accurate retrieval of information from large databases or documents. However, the specific details of how LangChain implements this functionality are not provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "response = qa.invoke(\"How does LangChain support document Q&A?\")\n",
    "print(\"Answer:\", response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341a76b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources:\n",
      "- LangChain enables document-based Q&A systems with FAISS and OpenAI.\n",
      "- LangChain is a framework to build applications powered by LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33107725",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Common RAG Challenges\n",
    "\n",
    "**1. Poor chunking strategy:**  \n",
    "Split documents intelligently ‚Äî use headers, sections, or semantic cues.\n",
    "\n",
    "**2. Low-quality embeddings:**  \n",
    "Clean input leads to better vector quality.\n",
    "\n",
    "**3. Retrieval depth (`k` too low):**  \n",
    "Try `k=3` or `k=5` for better context coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46af26",
   "metadata": {},
   "source": [
    "### ‚úÖ Wrap-Up\n",
    "You've built a production-ready RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
